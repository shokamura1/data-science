# 2019年3月11日 『Rによるデータサイエンス』 Ⅱ部 5章

# 『Rによるデータサイエンス』Ⅱ部 5章
# 5章の構成
#  5-3-1 k平均法
#  5-3-2 解析と結果
# 5-4 モデルに基づいたクラスター分析
#  5-4-1 モデルに基づいたクラスター分析とは
#  5-4-2 モデルの選択と解析
# 5-5 補遺と注釈


# 5章のゴール
# ・クラスター分析の概念を理解する
# ・非階層的クラスター分析の概念を理解し、Rで実装できるようになる

# 階層的クラスター法は個体数が多いと計算量が膨大になり、大量のデータ分析には向かない
# 大規模のデータセットのクラスター分析には、非階層的クラスター法が多く使われている
# 非階層的クラスター法の代表的な手法としてk平均法(k-means)法がある

# ●5-3-1 k平均法
# k平均法の大まかな流れは以下の通りである

# (1) k個のクラスター中心(seeds)に初期値を適当に与える
# (2) 全てのデータをk個のクラスター中心との距離を求め、最も近いクラスターに分類する
# (3) 形成されたクラスターの中心を求める
# (4) クラスターの中心が変化しない時点までステップ(2)(3)を繰り返す

# k平均法で多く用いられているのは、以下の通りである
# 1957年に提案されたLloyd法
# 1965年に提案されたForgy法
# 1969年に提案されたMacQueen法
# 1979年に提案されたHartigan-Wong法

# ●5-3-2 解析と結果
# (1) kmeans関数
# Rにはkmeans関数が定義されており、シンタックスは以下の通り

# kmeans(x, centers, iter.max = 10, nstart = 1, 
#        algorithm = c('Hartington-Wong', 'Lloyd', 'Forgy', 'MacQueen'))
        
# 引数xはデータを表す
# centersでは、クラスターの数、あるいはクラスターの中心を表す
# iter.maxでは繰り返しの最大値を表す
# nstartでは初期中心を与える方法を指定する

# centersがクラスターの数である場合、初期中心値はランダムに与える
# 引数algorithmには4つの方法('Hartington-Wong', 'Lloyd', 'Forgy', 'MacQueen')がある
# デフォルトにはHartington-Wongが指定されている
# 経験上、Hartington-Wongが他の方法よりより良いとされている

# kmeans関数の計算結果は以下のように出力される
# $clusterは、計算結果としてクラスターの分類結果を表す
# $centersは、クラスターの中心ベクトルを表す
# $sizeは、各クラスター内の個体数を表す

# (2) ケーススタディー
# ここでは階層的クラスター同様にseisekiのデータを使う
# kmeans関数では、データ以外にクラスター数を指定することが必要になる
# 以下にクラスター数を2に設定した結果を示す

(seiseki.km <- kmeans(seiseki, 2))
summary(seiseki.km)

# kmeans関数による分類結果と各クラスのサイズは次のコマンドで返す

seiseki.km$cluster

# この結果は、階層的クラスター分析(ユークリッド距離、最遠隣法)の結果と一致する


# ●5-4 モデルに基づいたクラスター分析
# ●5-4-1 モデルに基づいたクラスター分析とは
# モデルに基づいたクラスタリング(model-based clustering)は、混合分布によるクラスター分析、潜在クラスター分析とも呼ばれている
# モデルに基づいたクラスター分析は、観測データが異なる確率分布による混合分布であると仮定し、
# 個体が属するクラスのラベルをも隠れ変数として推定する
# 確率分布は理論上ではどのような分布でも良いが、現状では楕円分布が用いられている
# 楕円分布は、正規分布を拡張し、指数分布などより多くの分布をまとめた分布族である

# 今、n個の個体X = {x1, x2, ..., xn}はc個の異なるパラメーターΘ = {θ1, θ2, ...,θc}の確率分布から抽出されたとする
# 個体xiがパラメーターθjの確率分布に従うことをp(xi|θj)で表す
# モデルに基づいたクラスタリンクは、最適のp(X|Θ)を求める問題である為、
# n個の個体とc個の確率分布の最大対数尤度を求める問題に帰する
# パラメーターの推定にはEM(Expectation Maximisation)アルゴリズムが使われている


# ●5-4-2 モデルの選択と解析
# (1) 関連の関数
# パッケージmclustには、EMClust関数、hc、hclustなどモデルに基づいたクラスタリングに関する関数がある
# パッケージmclustはCRANミラーサイトからダウンロード可能

# ・EMClust関数
# EMClust関数は、最大尤度推測法を用いたEMアルゴリズムでパラメーターを推定する際に必要になる
# 混合分布モデルの情報量基準BIC(Bayesian Information Criterion)値を求める

# ・hc関数
# モデルに基づいた階層的クラスタリングを行い、hclass関数はクラスターを分類する
# hclass関数では、kmeans関数のようにクラスターの数を指定する必要有

# (2) ケーススタディー
# hclass関数を用いるには、クラスター数を指定する必要がある為、まずEMclust関数を使って、
# クラスター数を推測することにする。
# ここでは、irisの属性データを除いて使う
# パッケージmclustでは、plot関数を用いてEMClust関数が返すモデルのBIC値の折れ線グラフを作成出来る

install.packages('mclust'); library(mclust)
plot(EMclust(iris[, 1:4]))

# 通常、モデルを選択する場合、情報量基準の値が小さいモデルを良いモデルとして評価するが、
# EMClust関数のBICは通常の情報量基準と逆になっており、BICの値が大きいモデルが
# 良いモデルと評価するようになっている。

# hc関数では、混合分布の型(球、楕円球)、体積、形状と軸の向きが同じあるかどうかを
# 引数modelName = ' 'で指定するようになっており、以下の6種類から選択することが出来る。

# 'E':      1次元等分散
# 'V':      球型、1次元可変な分散
# 'EII':    球型、等体積
# 'VII':    球型、異なる体積
# 'EEE':    楕円球型、等体積・形状・向き
# 'VVV':    楕円球型、異なる体積・形状・向き

# BICが大きいモデルは、VEV、VVVであるが、hc関数にはVEVが実装されていないので
# VVVモデルを用いることにする

iris.hc <- hc(modelName = 'VVV', data = iris[, 1:4])

# 次のステップでは、混合分布VVVモデルに基づいて求めた結果を元にhclass関数でクラスター分類を行う。
# hclass関数ではクラスターの数を指定することが必要になる。
# BCIの値から、VVVモデルを用いた場合、クラスターの数は2, 3が考えられる。
# クラスターの数を3とした例を次に示す。

iris.hcl <- hclass(iris.hc, 3)

# 求めた結果の精度を確認するため、既知のクラスのラベルと分類結果とのクロス表を次に示す。
# 返された結果から分かるように、14個のvirginicaがversicolorとして誤分類されている。

table(iris[, 5], iris.hcl)

# clPairs関数を次のように用いると推定された結果の対散布図が作成される。

clPairs(iris[,1:4], cl = iris.hcl)


# ●5-5 補遺と注釈
# クラスター分析に関しては、パッケージclusterには以下の関数がある
# ・clara関数
# ・pam関数
# ・fanny関数
# ・agnes関数
# ・diana関数
# ・mona関数

# pam関数、clara関数、fanny関数はk平均法と同じく、分割化の方法による非階層的クラスター分析の結果を返す。
# pam関数はk平均法よりロバストであり、距離データを扱うことも出来る。
# また、その結果にplot関数を適応すると、散布図にクラスターごとに楕円で囲むグラフが作成される。

library(cluster)
plot(pam(iris[, 1:4], 3), ask = TRUE)
2

# 上記のplotコマンドを実行すると、3つの選択項目が返される。
# Selection:に数値1, 2, 3いずれか一つを入力し、Enterキーを押すとグラフが作成される。
# 作図の終了は、メニューのSTOPアイコンをクリックする。

# agnes関数、diana関数、mona関数は階層的クラスター分析関数である。
# mona関数は2値データの階層的クラスター分析を行う。
# これ以外にも、いくつかのパッケージにクラスター分析の関数がある。


# 以上