# 2019年4月15日 『Rによるデータサイエンス』 Ⅱ部 14章 ニューラルネットワーク

# 『Rによるデータサイエンス』Ⅱ部 14章　ニューラルネットワーク
# ●14章の構成
#  14-1 ニューラルネットワークとは
#  14-2 ニューラルネットワークの基礎
#   14-2-1 基本の概念
#   14-2-2 モデルの分類
#   14-2-3 単一中間層ニューラルネットワーク
#  14-3 パッケージとケーススタディー
#   14-3-1 パッケージと関数
#   14-3-2 ケーススタディー


# ●14章 ニューラルネットワークのゴール
# ・ニューラルネットワークの基本概念を理解する
# ・パッケージnnetを使って、Rで実装出来るようになる

# 本章では、目的変数を使うニューラルネットワークの基本概念及び、階層的ニューラルネットワークによる
# データ解析の内容を扱う。又、学習ベクトル量子化(LVQ)の例を示す。
# 解説量が多いことから、パッケージclassを使ったニューラルネットワークは別ファイルに分割した。

# ●14-1 ニューラルネットワークとは
# 我々、人間の脳には約140億個のニューロン(神経細胞)があり、それぞれのニューロンはある規則に従って結合され
# 神経回路を形成している。大脳皮質の薄い切片を肉眼で見た場合、見えるのは神経細胞の2%に過ぎないという。

# 神経細胞が結ばれた神経回路をモデル化したものを、人工ニューラルネットワーク、
# 通常は単にニューラルネットワーク(neural network)と呼ぶ。
# ニューラルネットワークは、非線形回帰分析、非線形判別分析(パターン認識)の有力な機械学習の方法である。


# ●14-2 ニューラルネットワークの基礎
# ●14-2-1 基本の概念
# ニューラルネットワークに関する研究の発端は、1943年のマカロックとピッツ(W.S. McCulloch and W.H.Pitts)の研究に遡ると言われる。
# その後、数回の研究ブームの起伏を経て研究が進められ、広い分野で実用されるようになった。

# 神経回路を構成する最少単位である神経細胞(ニューロン)は、細胞体、軸索、樹状突起、シナプス(Synapse)により構成されている。
# 細胞体は、他の細胞から送られた信号を処理し、ある条件を充たすと、次のニューロンに信号を送る。
# 軸策はニューロンの出力信号を次のニューロンに送る経路であり、樹状突起は信号を受け取り、細胞体に転送する経路である。
# 軸策、樹状突起は細胞体から多数分布した樹状の線維の集りであるが、
# 軸策は樹状突起より長く伸びている特徴がある。シナプスは軸策先に付いた粒状のもので、
# 他のニューロンの樹状突起と接続・切断する機能を果たす。
# ニューロンとニューロンはシナプスという接触点を通じて結び付けられ、
# 信号の転送を行う。軸策から流れる電気信号を神経インパルスと呼び、略してインパルスと呼ぶ。

# このようなニューロンが多数並列に接続されたシステムを数理的にモデル化したものがニューラルネットワークである。
# ニューラルネットワークの構成要素は形式ニューロン(ユニットとも言う)である。
# 形式ニューロンはニューロンの数理的モデルで、次のようなニューロンの機能に着目してモデル化している。

# ・樹状突起とシナプスによる情報の修飾
# ・細胞体内での信号の加算
# ・出力信号の生成

# 以下の解説は、形式ニューロンの構造図を参照している。
# 形式ニューロン構造図については、以下のリンクを参照のこと。

# ●形式ニューロン構造図
# https://images.app.goo.gl/SnpkNKFGHVNT42UX9

# x1, x2, x3, ..., xpは樹状突起による入力信号であり、w1, w2, w3, ...,wpは
# それぞれ入力信号に対応するシナプスの結合の重みである。
# 加算部ではそれぞれの入力値xiに重み、wiを掛けた代数和u = Σp i=1 wixiを求め、
# 出力部は出力信号f(u)を生成する。形式ニューロンでの入力と出力の関係は、通常次のような関数

# y = f(u) = {1 u >- Θ}
#          = {0 u < Θ}

# あるいはy = f(u) = 1/1 + e-u (ただしu = pΣi = 1 wixi)で表される。

# 出力関数y = f(u)はuの関数であり、uは入力変数xiと重みwiの線形結合である。
# ニューラルネットワークにおける結合の重みwiとしては、初めの段階ではランダムな値が与えられ
# 得られた結果を何らかの方法で評価を行い、結果が目標値に近づくように重みを換える計算を繰り返し、
# 徐々に最適な値に置き換えられる。このような目標値と比較しながら計算を繰り返し、最適値を求めることを、機械学習と言う。
# ニューラルネットワークは、このような形式ニューロンの相互結合により情報の転送・処理を行う。

# ニューラルネットワークが最も得意なのは、パターン認識・分類、及びノイズが混在しているデータの処理である。
# ニューラルネットワークが最も早く応用されたのは、手書き文字の識別を含む画像のパターン認識、
# 音声認識などの情報工学に関する分野であった。
# その後、医学の病気の診断、財務分析、経済分析、市場分析などにも応用されている。
# 最近では、人文科学への応用も見られるようになっている。


# ●14-2-2 ﾓﾃﾞﾙの分類
# 複数の形式ニューロンがネットワーク状に結合したものがニューラルネットワークであるが、
# その結合の構造が異なるとネットワークの機能と特徴も異なる。
# このネットワーク構造をネットワークモデルと言う。現時点で多く使われているネットワークモデルは、
# 階層型ネットワークと非階層型ネットワークに分けられる、又、学習法では教師有りの学習法と教師無しの学習法に分けられる。

# (1)階層型ネットワーク
# 階層型ネットワークは、最も多く使われているニューラルネットワークのモデルである。
# 階層型ネットワークは複数の形式ニューロンが階層的に結合されたものである。
# 書籍内で紹介されている図では、形式ニューロンが3層に並んでおり、入力層と中間層(隠れ層とも呼ぶ)の各ユニットの
# 出力は次の層の全てのユニットにリンクされている。
# このようなモデルを階層的完全結合型と呼ぶ。階層型は、全て完全結合とは限らない。
# また、入力層のユニットが中間層を跳び越えて直接出力層のユニットと結合することも可能である。
# 更に中間層は1層だけとは限らない。

# 階層型ネットワークは、信号の流れの立場からはフィードフォーワード(前向きのみ)である。
# 階層型ネットワークでは、中間層の層数及び中間層のニューロンの数が学習の回数及び
# 収束に直接影響を与えるため、中間層、及び中間層のニューロンの数をいくつにするかが問題になる。
# これらは、入力のデータの構造に依存するので、一概には言えない。
# 理論的には4層にした方が汎化能力の点で優れていると言われているが、計算の負担などを考えると
# 層数を少な目にした方が良い。一般的には、3層でかなりの問題が解けると言われている。

# (2)非階層型ネットワーク
# 階層型ネットワーク以外のネットワークモデルを非階層型ネットワークと呼ぶ。
# 非階層型ネットワークは、信号が任意の方向に流れることもある。信号が逆方向、
# またはループ状に流れることもある。


# ●14-2-3 単一中間層ニューラルネットワーク
# 単一中間層ニューラルネットワークは、次の式で定式化することが出来る。

# Yk = φ0(αk + Σh Whkφh (αh + Σi Wihxi))

# 階層型ネットワークにおける結合の重みを定めるための学習は、次のステップに分けられる。
# (1) 入力データx1, x2, ..., xpをニューラルネットワークに与え、最初段階では結合の重みとして
# w1, w2,...,wpに小さいランダム値を与えて出力を求める。

# (2) 出力結果と学習用のデータを比較し、新しい結合の重み
# w(j + 1) = w(j) + η * δ * Onet を計算する。

# 式内で使った各記号は次のように定義されている。
# w(j + 1):     j + 1回目の結合の重み
# w(j):         j回目の結合の重み
# η:            定数
# δ:            ニューロンの出力結果と学習用のデータとの差に関する関数であり、中間層、出力層によって異なる。
# Onet:         ニューロンの出力結果


# ●14-3 パッケージとケーススタディー
# ●14-3-1 パッケージと関数
# Rには中間層ありのニューラルネットワークのパッケージnnetがあり、Rをインストールする際に自動的にインストールされる。
# パッケージnnetは、単一中間層の階層型ニューラルネットワークのnnet関数を主としたパッケージである。
# nnet関数は、教師有りの学習法のアルゴリズムに基づいている。nnet関数を使って、学習を行い、
# 学習モデルによる当てはめは、predict関数を使う。
# nnet関数の書式は、次の2種類がある。

# nnet(formula, data, weights, size, ...)
# nnet(x, y, weights, size, ...)

# ■nnet関数の主な引数
# 引数          説明                    デフォルト
# formula       y~x1 + x2の形式
# data          データセットの名前
# x             説明変数のデータ
# y             目的変数のデータ
# subset        dataの中の一部分
# weights       個体の重み                1
# rang          初期の重みのランダム値の範囲[-rang, rang]   0.7
# Wts           結合の重みの初期値、指定しない時にはrang範囲の一様乱数を使う        乱数
# size          隠れ層のユニット数
# lineout       線形出力のユニット数      False
# entropy       エントロピー法を使用(最大条件付の尤度)    False
# softmax       対数確率モデルを使用      False
# decay         衰退重みのパラメーター    0
# maxit         最適化計算の繰り返し回数  100
# Hess          重みに関する最適なHessian値を返す   False
# trace         最適化計算の過程を出力    True
# MaxNWts       最大許される重みの数      1000
# abstol        当てはめ値による計算を止める制御値  1.0e-4
# reltol        最適値による計算を止める制御値      1.0e-8

# パッケージnnetにおけるpredict関数のシンタックスを以下に示す。

# predict(学習したモデル, データ, type = '')

# 引数typeに'raw'を指定すると当てはめの値を返し、'class'を指定すると対応するクラス
# (判別分析のグループ)に関する値を返す。


# ●14-3-2 ケースタディー
# ここでもirisのデータを使うことにする。
# まず学習用のデータ(iris.train)とテスト用のデータ(iris.test)を作成する。

even.n <- 2 * (1:75)
iris.train <- iris[even.n,]
iris.test <- iris[-even.n,]

# 作成した教師(学習、訓練)データを使って、次のようにnnet関数によるニューラルネットワークモデルを作成し、
# predict関数を使ってテストデータについて予測・判別を行う。

library(nnet)
iris.nnet <- nnet(Species~., size = 3, decay = 0.1, data = iris.train)
iris.nnetp <- predict(iris.nnet, iris.test[, -5], type = 'class')
table(iris.test[, 5], iris.nnetp)

# 返された結果から分かるように、上記のニューラルネットワークモデルによるirisの品種の識別問題では、
# 75の個体中、2つが誤識別されている。
# ニューラルネットワークのモデルは、使う引数のパラメーターに依存するため、上記の結果が最善の結果とは言い切れない。
# どのパラメーターをその値にすれば良いかについては、初心者にとっては頭を悩ます問題である。
# パラメーターの調整の問題はニューラルネットワークの問題だけではなく、機械学習の多くのアルゴリズムにおける共通の問題である。
# パラメーターの調整は数理的理論に基づいて指定できるものもあり、経験に頼るしかないものもある。

# nnet関数の回帰問題への適応は、関数の使用法は上記に示した分類の問題と同じであるが、
# 引数linout = Trueを加える必要がある。当てはめ値と残差は、それぞれfitted.value、residualsで呼び出す。
# predict関数を使うと、新しいデータの推測値を求められる。


# 以上