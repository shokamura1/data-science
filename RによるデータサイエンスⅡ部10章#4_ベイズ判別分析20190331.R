# 2019年3月26日 『Rによるデータサイエンス』 Ⅱ部 10章4篇

# 『Rによるデータサイエンス』Ⅱ部 10章4篇
# ●10章の構成
# 10-5 ベイズ判別法
#  10-5-1 ベイズ判別法の基礎
#  10-5-2 関数と判別分析のケーススタディー
# 10-6 補遺と注釈

# ●10章 ベイズ判別法のゴール
# ・ベイズ判別の概念を理解する
# ・ベイズ判別法をRで実装できるようになる


# ●10-5 ベイズ判別法
# ●10-5-1 ベイズ判別法の基礎
# 今、個体の判別に関する行列をX = {x1, x2, ..., xi, ..., xn},目的変数のベクトルを
# Y = {y1, y2, ..., yi, ..., yn}とする。
# 個体iの変数ベクトルはxi = [xi1, xi2, ..., xip]であり、gは個体が属するグループのラベルである。
# 通常グループの数gは、個体が属するグループのラベルである。
# 通常グループの数gは、個体の数nより大いに小さい(g << n)。

# 説明辺陬を条件、応答明変数を結果と考えた場合、条件Xのもとで結果Yが起こる条件付き確率p(Y|X)は乗法定理から
# p(Y|X) = p(Y)p(X|Y) / p(X) と表すことが出来る。

# 個体xjがグループycに属する確率p(Yc|Xj)は
# p(Yc|Xj) = p(Yc)p(Xj|Yc) / Σ(g, i = 1)p(Yi)p(Xi|Yi)で表される。
# これが通常、ベイズ定理と呼ばれている表記である。

# ベイズアプローチによる判別分析では、与えられた学習用のデータからp(Y|X)が最大となる確率モデルを求め、
# 所属不明のX~について、そのグループの所属を判別する。

# p(Y|X)を事後確率(posterior probability), p(Y)を事前確率(prior probability)と呼ぶ。
# 事後確率p(Y|X)を最大にすることは、p(X)が定数であるのでp(Y)p(X|Y)を最大にすることである。

# Xが質的データである場合は、p(Xc|Yc)は相対頻度を確率の推測値にすれば良いが、
# Xの中に連続の量的変数がある場合は、それを質的データに離散するか、ある確率分布に属するという仮定を置かざるをえない。

# 前者の場合は、量的のデータを質的データに離散化するときの区間の大小が問題であり、
# 後者はどのような確率分布を仮定すればよいかが問題である。
# 現実の問題では前者より後者の方が多く用いられ、正規分布、対数正規分布、ガンマ分布などが多く使われている。


# ●10-5-2 関数と判別分析のケーススタディー
# パッケージe1071やklaRなどにベイズ判別法として、ナイーブベイズ分類器(naive Bayse classifiers)　の関数が用意されている。
# 機械学習の問題では、学習によって分類を行うシステムを分類器(classifier)と呼ぶ。
# 分類器は分類の問題だけでなく、回帰問題、生存分析に対応する学習アルゴリズムの別名である。
# 文献によっては、分類器を仮説(hypothesis)あるいは学習器(learner)とも呼ぶ。

# パッケージe1071の関数naiveBayesは、標準的なアルゴリズムを使い、データが独立で正規分布に従うと仮定している。
# パッケージklaRのNaiveBayes関数は、e1071のNaiveBayes関数にカーネル法による確率密度の推測及び
# ユーザーによる確率分布の指定を行うオプションを加えたものである。
# NaiveBayes関数のシンタックスは以下の通り。

# NaiveBayes(formula, data,...)
# NaiveBayes(x, grouping, prior, usekernel = FALSE, ...)

# NaiveBayes関数には、確率密度をカーネル法で推定する引数usekernelがある。
# デフォルトでは、usekernel = FALSEである。
# usekernel = TRUEにすると、確率密度をデータから推定して用いる。
# データによっては大きな影響を与える。

# ここでは例として、パッケージmlbenchのガラス破片データGlassを使うことにする。
# ガラス破片データは、7種類のガラス破片について、次に示す9つの項目で、214の観測体に対して、
# 測定したデータフレームである。

# [, 1] RI refractive index(屈折の指数)
# [, 2] Na Sodium(ナトリウム)
# [, 3] Mg Magnesium(マグネシウム)
# [, 4] Al Aluminum(アルミニウム)
# [, 5] Si Silicon(シリコン)
# [, 6] K  Potassium(ポタシウム)
# [, 7] Ca Calcium(カルシウム)
# [, 8] Ba Barium(バリウム)
# [, 9] Fe Iron(鉄)
# [, 10] Type Type of Glass(グラスの種類)

# 求めたモデルで、新しいデータを判別する関数はpredict関数である。
# まず、密度関数を推定せずに判別を行った分析を表す。
# ただし、ここでは変数K, Ca, Ba, Feは除いて用いる。

install.packages('klaR'); library(klaR)
install.packages('mlbench'); library(mlbench)
data(Glass); G <- Glass[, c(1:5, 10)]
m1 <- NaiveBayes(Type~., data = G)
m1.p <- predict(m1)
tem1 <- table(G$Type, m1.p$class)
1 - sum(diag(tem1))/sum(tem1)   # 誤判別率


# 事前確率密度をカーネル法で推測して、判別を行う結果を示す。

m2 <- NaiveBayes(Type~., data = G, userkernel = TRUE)
m2.p <- predict(m2)
tem2 <- table(G$Type, m2.p$class)
1 - sum(diag(tem2))/sum(tem2)   # 誤判別率

# このケースでは、確率密度の推測にカーネル法を用いた結果の誤判別率が約27ポイントも下がっている。
# このデータは、問題に対して良すぎる例であり、特殊なケースだと考えた方が良い。
# ナイーブベイズ法は、決して判別率が高い分類器とは言えないが、
# このデータに限っては、比較的良いパフォーマンスを示している。
# ついでに線形判別方法とk-NN法の結果を次に示す。
# このデータでは、ナイーブベイズ法がk-NN法より誤判別率が低い。


m3 <- lda(Type~., data = G)
m3.p <- predict(m3)
tem3 <- table(G$Type, m3.p$class)
1 - sum(diag(tem3))/sum(tem3)

install.packages('class'); library(class)
set.seed(1)
m4 <- knn(G[, -6], G[, -6], G[, 6], k = 5)
tem4 <- table(G$Type, m4)
1 - sum(diag(tem4))/sum(tem4)

# ナイーブベイズ法は、データサイズが大きくなると計算に時間が掛かる短所を持っている。
# 推測された確率密度は、plot関数を用いて変数ごとにプロットすることが出来る。

# ●10-6 補遺と注釈
# 非線形判別分析には、多くの方法が提案されている。
# 本書で紹介した方法の中で最も多く使われているのはk最近傍法である。
# k最近傍法に関しては、パッケージclass以外に、重み付きのk最近傍法パッケージkknn、
# カテゴリカルデータのk最近傍法パッケージknncatなどがある。
# 又、パッケージklaRにはシンプルk最近傍法のsknn関数がある。

# 多変量データ解析の書物では、判別関数による判別分析を扱っているものが多い。
# これは判別分析の基礎であるからである。
# しかし、判別関数による判別分析はデータの変数が多く、複雑なデータ構造に対してはパフォーマンスが良くない。
# 近年、機械学習のアプローチによる非線形判別分析のアルゴリズムが多数提案されている。
# そのいくつかの主な方法は、13～16章で紹介する。


# 以上