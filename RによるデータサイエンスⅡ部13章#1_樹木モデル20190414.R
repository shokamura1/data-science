# 2019年4月10日 『Rによるデータサイエンス』 Ⅱ部 13章 樹木モデル

# 『Rによるデータサイエンス』Ⅱ部 13章 樹木モデル
# ●13章の構成
# 13-1  樹木モデルとは
# 13-2  樹木モデルの基礎
# 13-3  パッケージとケーススタディー
#  13-3-1  パッケージrpartとmvpart
#  13-3-2  分類木

# ●13章 樹木モデルのゴール
# ・樹木モデルの基本概念を理解する
# ・パッケージrpartを使って、Rで実装出来るようになる

# 本章では、データマイニングの中で広く使われている樹木モデルに基づいた予測あるいは
# 判別を行う回帰・決定木の基本概念、及び木の生成、木の剪定、多変量回帰木などについて解説する。
# 例によって解説量の多さから、決定木の分析手法毎にファイルを分割して、アップする。


# ●13-1 樹木モデルとは
# 樹木モデル(tree-based model)は非線形回帰分析、非線形判別分析の1つの方法であり、
# 回帰問題では回帰木(regression tree)、分類問題では分類木(classification tree)あるいは
# 決定木(decision tree)と呼ばれている。

# 樹木モデルでは、説明変数の値を何らかの基準を元に分岐させ、判別・予測のモデルを構築する。
# 分岐の過程は木構造で図示ことが出来、またIf - Thenにような簡潔なルールで表すことができる。
# 樹木モデルは理解しやすいことから、その応用が急速に広がっている。


# ●13-2 樹木モデルの基礎
# 樹木モデルに関する研究は、1960年代初期までさかのぼる。
# 今日、広く使われているのはCHAID、C4.5/C5.0/See5、CARTをベースとしたアルゴリズムである。
# CHAID(CHi-squared Automatic Interaction Detection)は、Hartiganが1975年に
# Morganらによって提案されたAID(Automatic Interaction Detection)を発展させたもので、
# 分岐基準としてカイ2乗統計量やF統計量などが使われている。

# C4.5/C5.0/See5は、オーストラリアのJ Ross. Quinlinが機械学習のアプローチで
# 1986年に発表したID3(Interactive Dichotomiser 3)を改良・発展させたものである。
# C4.5/C5.0/See5の前身であるID3では、分岐の基準として情報利得(Information Gain)を使ったが、
# C4.5/C5.0/See5では、利得比(Gain Ratio)を使っている。
# C4.5/C5.0/See5は2進木にならないのが1つの特徴である。

# CART(Classification And Regression Tree)は、カルフォルニア大学のL. Breinman、R.A. Olshen
# C.J.Stone、スタンフォード大学のJ.H.Friedmanが1970年代頃から共同研究を始め、
# 1980年代初め頃に公開したアルゴリズムである。

# CARTでは、説明変数を2進木に分岐させる、初期のCARTでは、分岐の基準としてジニ係数を
# ジニ多様性指標(Gini's diversity index)として使ったが、最近は情報利得(Information Gain)なども使われている。
# CARTは樹木を予め何の制限もなく成長させ、データと対話しながら剪定する方法を取っている。
# 樹木の剪定とは、成長し過ぎた樹木を何らかの基準に基づいて枝狩りし、
# 当てはめの良い簡潔な樹木モデルを構築する方法である。

# これらの樹木モデルのアルゴリズムの大きな違いは、樹木の生成・成長、樹木の剪定のアルゴリズムである。
# 樹木の生成・成長とは、データセットから樹木の幹･枝となる説明変数を選定し、
# 分岐基準を元に分岐させ、樹木を成長させることである。

# 樹木を生成する際に、どの変数のその値を木の枝分けする分岐点(ノード)にするかに関しては、
# アルゴリズムによって用いる計算が異なる。

# CARTでは、エントロピー(entropy)とジニ係数(GI: Gini Index)を使って分岐点を計算する。
# エントロピーとジニ係数の定義を次に示す。これを分布の不純度の尺度とも呼ぶ。
# 式の中のtはノード、iはクラス、pは分割された個体がクラスに属する比率である。

# entropy = - cΣi=1 p(i|t)log2 p(i|t)
# GI = 1 - cΣi=1 [p(i|t)]2

# 実際の書籍では、以降具体例を交えた樹木モデルのアルゴリズム解説が掲載されているが、
# 紙面の関係上、割愛しパッケージとケーススタディーの章に移る。


# ●13-3 パッケージとケーススタディー
# ●13-3-1 パッケージrpartとmvpart
# Rには樹木モデルを構築するパッケージがいくつかある。ここではパッケージrpartとmvpartを使って解説する。
# パッケージrpartはRをインストールする際に自動的にインストールされる。
# パッケージmvpartはrpartの機能を拡張した上位パッケージである。
# mvpartはCRANミラーサイトからダウンロード出来る。樹木モデルを構築するメイン関数rpartのシンタックスを以下に示す。
# より多くの引数やデフォルトの設定などはhelp(rpart)で確認できる。

# rpart(formula, method,)

# formulaの書式は、回帰分析や判別分岐の関数と同じく目的変数と説明変数を指定する。
# 引数methodのオプションを以下の表に示す。引数methodを指定しない場合は、formulaに指定されている応答変数のデータ型で判断する。
# 関数オブジェクトrpartに関連する関数(同じクラスに属するメソッド)を以下に示す。

# ■引数methodのオプション
# 応答変数y               引数の書式
# yが一般の量的変数       method = 'anova'
# yがポアソン分布         method = 'poisson'
# yが質的変数             method = 'class'
# yが行列                 method = 'mrt'
# yが距離                 method = 'dist'
# yが生存データオブジェクト     method = 'exp'

# ■rpartクラスの主な関数
# 関数名            略式            主な機能
# plot.rpart        plot            樹木の図示
# text.rpart        text            文字列の操作
# plotcp            plotcp          交差確認の結果の図示
# predict           predict         モデルによる当てはめ
# print.rpart       print           樹木の出力
# printcp           printcp         複雑さのパラメータの出力
# prune.part        prune           樹木の剪定を行う
# residuals.rpart   residuals       残差を返す
# rpart.control     control         樹木をコントロール
# summary.rpart     summary         要約を返す


# ●13-3-2 分類木
# rpart関数では、分岐点の計算はジニ係数とエントロピーを使っている。
# デフォルトにはジニ係数が設定されている。エントロピーを使う場合は、
# 引数split = 'information'を使う。

# (1)木の生成
# rpart関数を使った木の生成について、例を使って説明する。
# ここでもirisのデータと、パッケージmvpartを使うことにする。

install.packages('rpart'); library(rpart)
set.seed(20)
iris.rp <- rpart(Species~., data = iris)

# 上記のコマンドを実行すると、デフォルト値に基づいたrpartの分類木が作成される。
# 作成された木の結果は、次のようにprint関数で返すことが出来る。
# print関数に使ったdigitは、返す値の小数点以下の桁数を指定する引数である。
# 返される結果はdigitの数値より1桁多い。

print(iris.rp, digit = 1)

# この結果のグラフは、次のようにplot関数とtextを使って作成することが出来る。
# パッケージmvpartがインストールされていないと、図のスタイルが若干変わることがある。

plot(iris.rp, uniform = T, branch = 0.6, margin = 0.05)
text(iris.rp, use.n = T, all = T)

# plot.rpart(以降plotと略す)関数には、樹木のデザインのための多くの引数が用意されている。
# 上で使ったuniformは、ノート間の間隔に関する引数である。
# デフォルト(uniform = FALSE)の設定は、木のノード間の間隔は分類のエラーの数に比例するように作成する。
# 引数uniform = TRUEにすると、ノード間の間隔は等間隔になる。

# 引数branchは、枝の角度を調整する。指定する値は、0から1までである。
# 0の場合の角度が最も大きく、1の場合は垂直になる。
# デフォルトはbranch = 1になっている。引数marginは図の外側の余白(マージン)を調整する。
# 引数marginの値が大きければ大きいほど、図が小さく、余白が大きくなる。

# text関数にも複数の引数が用意されている。
# 上記のコマンドで使った引数use.nは、各ノードに含まれる個体の数の表示を設定する。
# デフォルトには、use.n = FALSEが設定されている。引数がuse.n = TRUEの場合は、
# ノードに含まれる数を表示する。

# (2)木の剪定
# rpart関数は、樹木を成長させると同時に交差確認法の結果も計算する。
# printcp関数でその結果を呼び出すことが出来る。デフォルトのn重交差確認の
# nはxval = 10に設定されている。
# printcp関数は、樹木の剪定のための複雑さのパラメーター(cp)を返す。
# この結果は、用いたデータセットをランダムに分割して、交差確認を行った結果であるので、
# 同じデータを使ってrpartを繰り返した場合、全く同じ結果になるとは限らない。
# モデルiris.rpを生成するには、同じ結果を出すため、乱数の種set.seedを使っている。

printcp(iris.rp)

# 通常、木のサイズはxerrorの最小値を中心としたその標準偏差1倍の範囲内の最大のxerrorを選ぶ。
# これをMin + 1SE法と呼ぶことにする。
# 上記の例で、返された最終行のxerro、xstdはそれぞれ0.11、0.031927であり、
# Min + 1SE = 0.1 + 0.031927 = 0.131927となる。
# この値は、第2行のxerror = 0.68より小さく、第3行のxerror = 0.11より大きいので
# 第3行のcp(complexity parameter) =0.02が1つの目安になる。

# rpart関数では、複雑度cpで木をコントロールすることが出来る。
# cpの値が小さいほど、木が複雑になる。デフォルトではcp = 0.01担っている。
# パッケージrpartには、rpart関数で生成させた木を剪定するprune関数がある。
# prune関数を使ってcp = 0.02で剪定したrpartの樹木を示す。

iris.rp1 <- prune(iris.rp, cp = 0.02)
plot(iris.rp1, uniform = T, branch = 0.6)
text(iris.rp1, use.n = T)

# パッケージrpartには、cpを視覚化するplotcp関数がある。
# plotcp関数は、樹木の剪定に必要となる交差確認に関する情報のプロットを返す。

plotcp(iris.rp)

# 下の横軸がcpで、上の横軸が木のサイズ(葉の数)である。
# 図の中の水平直線Min + 1SEはxerrorの最小値に1倍の標準偏差を足した値である。
# その直線の下方に最も近い点をオレンジ色、xerrorの最小値の点を赤い色で示す。
# オレンジ色の点が樹木を剪定する目安である。

# cp = 0.094、木のサイズが3に対応するところがオレンジ色である。
# この結果とplotcpでの剪定の目安cp = 0.02は一致していない。
# これはprintcpでは分岐の数とcpの対応関係を示し、plotcpでは木のサイズとcpの対応関係であるからである。
# 木の剪定には、どの結果を使っても良い。

# (3)判別
# 作成した樹木のモデルを使って未知のデータについて予測・判別を行うためには、
# モデルの作成に使っていないデータが必要になる。そこで、ここでもirisデータを奇数行と偶数行に分けて
# 学習用とテスト用にする。

even.n <- 2*(1:75) - 1
iris.train <- iris[even.n,]
iris.test <- iris[-even.n,]
set.seed(20)
iris.rp2 <- rpart(Species~., iris.train)
plotcp(iris.rp2)

# この結果とiris.rp2の木を比較すると、さらに剪定する必要がないことが分かる。
# 作成したモデルを使った予測・判別にはpredict関数を使う。

iris.rp3 <- predict(iris.rp2, iris.test[, -5], type = 'class')
table(iris.test[, 5], iris.rp3)


# (4)コントロール
# rpart関数が返す木は、rpart.control関数によりコントロールされている。
# その引数の説明は、help(rpart.control)で確認できる。そのシンタックス及び引数のデフォルト値は以下の通り。

# rpart.control(minsplit = 5, minibucket = round(minsplit/3), cp = 0.01, 
# maxcomplete = 4, maxsurrogate = 5, usersurrogate = 2, xval = 10,
# surrogatestyle = 0, maxdepth = 30, ...)

# 引数minsplitは分岐する際に、ノードに含まれる最小の個体数をコントロールする。
# パッケージrpartとmvpartのデフォルト値はそれぞれ20と5に設定されている。
# 値が小さいほど、大きな木が生成される。引数minsplit以外の引数のデフォルト値はパッケージrpartとmvpartは同じである。

# 引数minbucketは葉における最少の個体数をコントロールする。
# デフォルトは、minsplitの1/3になっている。minsplitとminbucketの中で1つのみが指定される時には
# 3 * minbucket = minsplitとする。

# 引数cp(complexity parameter)は、既に説明したように複雑さのパラメーターである。
# 明らかに勝ちのない木が成長しすぎることを制御するパラメーターである。
# デフォルトは0.01になっている。値が小さいほど、木が茂る。

# 引数maxcompete、maxsurrogate、usesurrogate、surrogatestyleは分岐の候補の出力、
# 欠損値の扱いなどに関連する引数である。これらは中・高級者向けである。
# 結果はsummarry関数から確認できる。

# 引数xvalはn重交差確認のnを設定する。
# デフォルトは10になっている。通常、3～10の値を指定する。
# 引数maxdepthは、木の最大の深さをコントロールする。根の深さは0をカウントされる。
# デフォルトは30になっているが、32ビットマシンではこれ以上設定することは意味がないだろう。

# rpart.control関数の引数は、rpart関数で直接、引数として使うことが出来る。


# 以上