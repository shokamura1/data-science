# 2019年3月24日 『Rによるデータサイエンス』 Ⅱ部 9章

# 『Rによるデータサイエンス』Ⅱ部 9章
# ●9章の構成
# 9-1 判別分析とは
# 9-2 線形判別分析の基礎
# 9-3 ケーススタディー
#  9-3-1 用いるデータ
#  9-3-2 lda関数
#  9-3-3 学習の結果と判別
# 9-4 交差確認
# 9-5 補遺と注釈


# ●9章 判別分析のゴール
# ・判別分析の概念を理解する
# ・判別分析をRで実装できるようになる
# ・交差確認の概念を理解する
# ・交差確認をRで実装できるようになる

# 本章では目的変数が質的データの教師データを用いる判別分析及びパターン認識の最も基本的な方法
# 線形判別分析方法と交差確認・検証法などについて解説する。

# ●9-1 判別分析とは
# 私達は、五感を通じて入力される膨大なデータを処理している。
# その中で最も多いのは、識別(discrimination)、分類(classification)、認識(recognition)に
# 関する処理である。
# 例えば、新聞や本などを読む時は、視覚を通じて入力されたデータと脳の中のデータ(学習済みデータ)との照合を行い、
# その文字の読みかたや意味などを識別・認識する。

# 識別に関する能力は人間のみならず、他の動物も持っている。
# 識別・認識に関することを機械的に実現する研究分野がパターン認識(pattern recognition)である。
# パターン認識の例としては、郵便番号による手紙の自動分類や指紋・顔の機械的な照合などが挙げられる。

# パターン認識では、コンピューターに記憶させたデータと認識すべきデータとの一致度を何らかのモデルによって計算する。
# その中で最も古典的な分析手法は判別分析(discriminant analysis)である。

# 判別分析は、固体(あるいは対象)が、どのグループに所属するかが明確である学習データを使って、
# 判別モデルを構築し、そのモデルを使って所属不明の個体(テスト用のデータ)がどのグループに所属するかを
# 判別する方法である。

# 判別分析は、線形判別分析と非線形判別分析に大別される。
# 又、所属不明の個体が2つのグループのいずれかに所属するかを判別する問題を2郡判別問題、
# 3つ以上のグループのいずれかに所属するかを判別する問題を多郡判別分析と呼ぶ。

# 判別分析に適しているデータの例として、Rに予め実装されているirisデータの一部分を示す。
# 最後のSpecies列が各個体のグループ情報、つまり外的規範になる。

iris[c(1:3, 51:53),]

# 回帰分析の場合は、外的基準が量的データであるのに対して、判別分析の外的基準は、
# 質的データである。機械学習の分野では、外的基準を用いた学習を教師有り学習、あるいは
# スーパーヴァイズド学習(supervised learning)と呼ぶ。


# ●9-2 線形判別分析の基礎
# 線形判別分析(linear discriminant analysis)はグループ分けの境界が直線あるいは超平面である場合、
# 次のような線形関数を使って、グループの所属を判別する方法である。

# 判別関数 = a0 + a1x1 + a2x2 + … + apxp

# フィッシャー(Fisher)が1930年中頃に提案した線形判別関数では、データが多変量正規分布に従い、
# グループの部ボンさんが等しいという仮定の下で、郡間の分散と郡内の分散の比を最大化することで、係数aiを求める。


# ●9-3 ケーススタディー
# ●9-3-1 用いるデータ
# ここではirisを使って3郡判別分析を行う。
# まずデータirisから、学習用のデータ(教師データ、訓練データとも呼ぶ)とテスト用データを作成する。
# まずirisデータのSpeciesのデータを簡潔にし、次の学習データとテストデータを作成する。
# 学習用データの作り方は色々と方法があるが、ここでは奇数行と偶数行に分けることにする。
# iris.trainを学習用データ、iris.testをテスト用データにする。

iris.lab <- c(rep('s', 50), rep('c', 50), rep('v', 50))
iris1 <- data.frame(iris[,1:4], Species = iris.lab)
even.n <- 2 * (1:75) - 1
iris.train <- iris1[even.n,]
iris.test <- iris1[-even.n,]

# ●9-3-2 関数lda
# RのパッケージMASSには、線形判別分析のlda関数がある。
# lda関数の最も簡潔なシンタックスは以下である。

# lda(formula, data)

# formulaでは「グループの識別変数 ~ 変数」のように記述する。
# 学習データセットを次のように用いると、判別分析に必要となる統計量が求められる。
# ただし、以下の書式は学習データセットの中ノア変数を全て用いた場合の書き方である。
# 学習データセットの中の変数の一部部だけを用いる場合は、y ~ x1 + x3のように
# 用いる変数のラベルを記号「+」でつなぐ。

library(MASS)
(Z.lda <- lda(Species~., data = iris.train))

# 返された線形判別係数(coefficients of linear discriminants)を用いて、判別関数を構築する。
# 判別関数は用いる変数と判別係数との線形結合である。
# この問題では、2組の判別係数(LD1, LD2)が返されている。LD1が第1判別関数の係数、
# LD2が第2判別関数の係数である。返された結果を、小数点以下３桁まで丸めた値を用いた第1判別関数を次に示す。
# 判別勘数式の中のx1, x2, x3, x4はそれぞれirisデータの変数Sepal.Length, Sepal.Width, Petal.Length, Petal.Widthを示す。

# fLD1 = -0.592x1 - 1.842x2 + 1.653x3 + 3.564x4 - c

# 定数項cはグループの平均と判別係数との線形結合の平均値で、次のようにして求めることが出来る。
# 第1列の値は第1判別関数の定数項で、第2列は第2判別関数の定数項である。

apply(Z.lda$means%*%Z.lda$scaling, 2, mean)

# 判別関数で得られた値を判別得点(discriminant score)と呼ぶ。
# 第1判別関数の判別得点は、以下のコードで求めることが出来る。

Z.lda$scaling[,1]%*%t(iris.train[,1:4]) -1.486146

# 各判別関数が全体のグループ間の分散をどのくらい説明できるかは、グループ間の分散の比率から読み取れる。
# 第1判別関数に含むグループ間の分散の比率(proportion of trace LD1 0.9913)は、0.9913である。
# これは第1判別関数の分散がグループ間の分散の99.13%を説明出来ることを意味する。


# ●9-3-3 学習の結果と判別
# 学習データに基づいて求めた判別結果はpredict関数を用いて呼び出すことが可能である。
# predict関数は$class, $posterior, $xを返す。

# $classは各個体が判別されたグループのラベルで、$posteriorは各個体がどのグループに判別されているかに
# 関する確率(0 ~ 1), $xは各個体の判別関数得点である。
# 学習データ￥における判別結果は、次のクロス表で確認することが出来る。

table(iris.train[, 5], predict(Z.lda)$class)

# Cグループの1つがVグループに、Vグループの一つがCグループに誤判別され、
# 誤判別率は2/75(2.67%)である。

# 判別関数得点をグラフに示すことも出来る。
# 以下のコマンドで、グループ毎の第1判別関数得点のヒストグラムが作成される。

plot(Z.lda, dimen = 1)

# 作成されたヒストグラムから分かるようにS(Setosa)は、C(Versicolor)、V(Virginica)と重ならないが、
# Cの右辺とVの左辺は若干、重なる。重なる領域が多いほど、お互いに間違って判別される確率(誤判別)が高い。

plot(Z.lda, dimen = 2)

# 引数dimen = 2を用いると、横軸を第1判別関数、縦軸を第2判別関数とした散布図が作成される。
# 判別関数が3つ以上の場合はｍdimenの値を3以上にすると、対散布図が作成される。
# 少し、工夫をすると判別直線を加えた散布図を作成できる。

# (2) テストデータの判別
# 求めた判別関数を使って所属不明のデータについて、判別を行う時には、predict関数を使う。

Y <- predict(Z.lda, iris.test[, -5])

# どれくらい正しく判別されているかは、テストデータのグループラベルと判別結果のグループラベルのクロス表を作成することが確認できる。

table(iris.test[, 5], Y$class)

# 上記の結果から分かるようにテストデータの中のS(Setosa)は、全て正しく判別され、
# C(Versicolor)は、1つがV(Virginica)と誤判別され、Vは2つがCと誤判別されている。
# 誤判別されているのは3つで、全体の中で占める割合(誤判別率)は3/75 = 0.04(4%)、
# 正判別率は1 - 0.04 = 0.96(96%)である。
# テストデータの判別得点の散布図を用いて、判別状況を視覚的に考察することも出来る。

plot(Y$x, type = 'n')
text(Y$x, labels = iris.test$Species)

# このアプローチによる線形判別分析は、各グループの母分散が等しいとの仮定に基づいている。
# 各グループの母分散が異なる場合は、マハラビノス距離による判別のような母分散に制約がない方法を用いたほうが良い。


# ●9-4 交差確認
# 本章では、判別分析を説明するために、irisデータを奇数行と偶数行に分けて、
# 学習用のデータとテスト用のデータに2分した。
# データセットから学習用のデータとテスト用のデータに分けて、モデルの構築・テストを行なうのに
# 交差確認(cross validation: 交差検証、交差妥当化とも呼ばれる)という方法がある。

# 交差確認法では、データセットの標本全体をn等分に分割し、そのうちの1等分をテスト用のデータ、
# それ以外のn - 1等分を学習用のデータとする。

# データセットをn等分した時、n重交差確認(n-fold cross validation)法と呼ぶ。
# n重交差確認法では、重複しない組み合わせでn回のモデルの構築とテスト(確認と検証)を行い、
# そのn回のテスト結果の平均を全体の評価に用いる。

# lda関数ではデータセットから1つの個体を除いて学習を行い、学習データに使っていない
# 1つの個体で判別モデルの評価を行なう作業を、全ての個体にたいして繰り返す。
# 交差確認(leave-one-out cross-validation)の引数CVが用意されている。
# これはn重交差確認のnが個体の数に等しい特殊なケースである。

# デフォルトではCVはFALSEになっている。
# CV = Trueにすると1つを除いた交差確認による結果が返される。
# 以下はirisデータを使った1つを除いた交差確認の結果である。

iris.CV <- lda(Species~., data = iris, CV = TRUE)
(lda.tab <- table(iris[, 5], iris.CV$class))

# 判別率と誤判別率は次のように求めることが出来る。

sum(lda.tab[row(lda.tab) == col(lda.tab)])/sum(lda.tab)

sum(lda.tab[row(lda.tab) != col(lda.tab)])/sum(lda.tab)

# 個体数がそれほど大きくならないデータセットに対する線形判別分析では、1つの個体を除いた
# 交差確認法が用いられているが、データマイニングの分野では、一般のn重交差確認法が多く用いられている。
# 尚、判別関数ldaにはleave-one-out cross-validation以外の交差確認の機能は用意されていない。


# ●9-5 補遺と注釈
# 線形判別分析は、等分散の制約条件があることと、大量の変数には向いていない短所が有り、
# 非線形判別分析法におされ気味である。
# lda関数に関連する関数としては、パッケージklaRの中の局所線形判別分析のloclda関数ｍ
# パッケージipredの中のslda関数などがある。


# 以上