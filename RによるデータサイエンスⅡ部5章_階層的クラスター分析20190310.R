# 2019年2月24日 『Rによるデータサイエンス』 Ⅱ部 5章

# 『Rによるデータサイエンス』Ⅱ部 5章
# 5章の構成
# 5-1 クラスター分析とは
# 5-2 階層的クラスター分析
#  5-2-1 階層的クラスター分析プロセス
#  5-2-2 クラスターの形成とコーフェン行列
#  5-2-3 階層的クラスター分析の諸方法
#  5-2-4 階層的クラスター分析のケーススタディー

# 5章のゴール
# ・クラスター分析の概念を理解する
# ・階層的クラスター分析の概念を理解し、Rで実装できるようになる
# ・階層的クラスター分析の諸方法の違いを理解し、Rで実装できるようになる
# ・階層的クラスター分析の結果を樹形図で現すことが出来るようになる


# 本章では、外的基準を持たないデータをグループ分けするクラスター分析方法を説明する
# まず距離、類似度を用いる階層的クラスター分析法を説明し、次に指定したグループの数に基づいて
# グループ分けするk平均法、最後に確率分布を仮定したモデルに基づいてグループ分けする方法を説明する

# ●5-1 クラスター分析とは
# 物事を整理整頓するには、機能、形状などの似ているものを同じところに集めて整理する
# データも同じく、データ構造の側面から似ている個体(あるいは多数)を同じグループに仕分けることが必要な場合がある
# データサイエンスにおける分類方法は、学習データがある分類方法と、学習データがない分類方法に大別される

# ここでいう学習データとは、どの個体がどのグループ(あるいはクラスとも呼ぶ)に属するかに関する外的基準である
# 外的基準がある場合の分類方法は、どの個体がどのグループに属するかが既知であるデータから、分類に関するモデルを作成し、
# そのモデルに基づいて、グループの属性が未知であるデータを最も類似すると判断されるグループに割り当てる
# 分類方法である。外的規準があるデータを訓練データ、あるいは教師有データと呼ぶ

# 学習データがない分類方法は、どの個体がどのグループに属するかに関する事前情報がないデータについて
# 分類する方法で、外的規準がない分類法とも言う
# 広義では主成分分析、因子分析、対応分析、多次元尺度法も外的規準なしの分類方法とも考えられるが、
# 本章ではクラスター分析を紹介する

# クラスター分析とは、データのパターンが似ている個体を同じグループにまとめる分析方法である
# 本章ではグループの形成状態を樹形図(デンドログラム)で示す階層的クラスター分析の方法と
# どの個体がどのグループに属するかを示す非階層的クラスター分析の1つであるk-means法、
# モデルに基づいたクラスター分析法について紹介する


# ●5-2 階層的クラスター分析
# 階層的クラスター分析とは、個体間の類似度あるいは非類似度(距離)に基づいて、
# 最も似ている個体から順次に集めてクラスターを作っていく方法である
# 階層的クラスター分析方法は、クラスターが作られていく様子を樹形図で示すことが出来る
# 樹形図はデンドログラムとも呼ばれる

# 樹形図は、逆さにした木の構造に似たグラフであることから、ラベルが付いている
# 部分を「葉(Leaf)」と呼ぶ。
# 葉と葉の距離が短いほど、個体が似ていると解釈する
# 樹形図では、いくつかの個体が階層的に集って1つのクラスター(房、枝)を形成し、
# 複数のクラスターが最終的には一つのクラスター(木)となる様子が観察できる

# 樹形図をある高さ(距離)のところで、直線を引いて切断することによって、クラスターの数と個体の分類が定まる
# 階層的クラスター分析は、クラスタリング法、凝集型階層手法とも呼ばれている


# ●5-2-1 階層的クラスター分析プロセス
# 階層的クラスター分析には、いくつかの方法があるが、いずれも次のようなステップを踏む

# 1. データから距離(類似度)を求める
# 2. クラスター分析の方法(最近隣法、最遠隣法など)を選択する
# 3. 選択された方法のコーフェン(cophenetic)行列を求める
# 4. コーフェン行列に基づいて樹形図を作成する
# 5. 結果を検討する

# ステップ1.では多次元尺度法を説明する際に示した距離(あるいは類似度)をデータから求める
# ステップ2.では、次の節で紹介する階層的クラスター分析方法を選択する
# ステップ2.で指定したクラスター分析方法で計算されるクラスター間の距離行列をコーフェン距離と言い、
# その距離行列をコーフェン行列と呼ぶ


# ●5-2-2 クラスターの形成とコーフェン行列
# 階層的クラスター分析では、データからの距離の行列を求め、距離の行列からコーフェン行列を求め、
# コーフェン行列に基づいて樹形図を描くステップを踏む

# 距離行列からコーフェン行列を生成する方法はいくつかあるが、第1段階は全て同じで
# 最も距離が近い2つの個体間の距離をコーフェン距離とする
# 第1段階が終わった後、どのようにコーフェン距離を求めるかはクラスター分析によって異なる
# ここでは具体的な例を用いて、その過程の一部分を説明する
# 因子分析を説明する際に用いた7人の5教科の成績データを用いることにする

seiseki.d <- dist(seiseki)
round(seiseki.d)

# 得られた距離データからコーフェン行列の計算過程の主なステップを解説する
# 上記の距離行列では、最も距離が近いのは吉野と佐藤12である
# そこでこの2人がまずクラスターc1{吉野, 佐藤}を形成する

# 次に距離の値が小さいのは、川端と鈴木の距離である
# 川端と鈴木がクラスター{川端, 鈴木}を形成すべきか、それともc1に川端を加えたクラスター{c1, 川端}、
# c1に鈴木を加えたクラスター{c1, 鈴木}が新しいクラスターを形成すべきであるかは、何かしらの
# ルールに従って計算する必要がある

# 説明を簡単にするため、この7人の距離関係を平面上で示すことが出来るとする
# 川端と鈴木の距離は既に分かっているので求める必要がない
# 問題はc1と川端の距離、c1と鈴木の距離をどのように求めるかである
# c1と川端の距離を考える場合でも、いくつかの方法が考えられる

# この問題では、どのような方法をとっても明らかに川端と鈴木の距離が短いので、
# 新しいクラスターc2は{川端, 鈴木}となる
# 続いて距離の値が小さいのは田中と川端である
# そこで田中と川端が属するクラスターc2、田中とクラスターc1の距離を計算する
# 田中とc2の距離が田中とc1の距離より近いのでc2に田中が加えられ、
# 新たなクラスターc3{c2, 田中} = {川端, 鈴木, 田中}が形成される
# 階層的クラスター法はこのようにクラスターを形成する

# このようにクラスターを形成する際には、クラスターとクラスターとの間の距離を計算する必要が出てくる
# 例えばクラスターc1とc3の距離を考えてみよう
# クラスター間の距離は、クラスター間で最も距離が近い個体同士の距離をクラスター間の距離にする(最近隣法)、
# 逆に最も距離が通し個体同士の距離をクラスター間の距離にする(最遠隣法)などの方法がある

# このように何らかの方法で求めた距離行列がコーフェン行列である
# 階層的クラスター分析の樹形図はコーフェン行列に基づいて作成する


# ●5-2-3 階層的クラスター分析の諸方法
# 階層的クラスター分析の方法は、コーフェン距離を求める方法を指す
# 本書では、最近隣法、最遠隣法、郡平均法、メディアン法、重心法、ウォード法について解説する

# (a) 最近隣法
# 最近隣法(nearest neighbour method)は、最短距離法、単連結法(single linkage)法とも呼ばれる
# 最近隣法は、2つのクラスターのそれぞれの中から、1個ずつ個体を選んで個体間の距離を求め、
# それらの中で最も近い個体間の距離を、この2つのクラスター間の距離とする方法である

# (b) 最遠隣法
# 最遠隣法(furtherst neighbour method)は、最遠距離法、完全連結(complete linkage)法とも呼ばれる
# 最遠隣法は、最近隣法とは逆に、2つのクラスターの中のそれぞれの中から1個ずつ個体を選んで個体間の距離を求め、
# それらの中で最も遠い個体間の距離を2つのクラスター間の距離とする

# (C) 郡平均法
# 郡平均法(group average method)は、最近隣法と最遠隣法を折衷した方法で、
# 2つのクラスターのそれぞれの中から1個ずつ個体を選んで、個体間の距離を求め、
# それらの距離の平均値を2つのクラスター間の距離とする

# (d) 重心法
# 重心法(centroid method)は、クラスターのそれぞれの重心(例えば、平均ベクトル)を求め、
# その重心間の距離をクラスターの距離とする。重心を求める際には、クラスターに含まれる個体数が反映されるように
# 個体数を重みとして用いる

# (e) メディアン法
# メディア法(median method)は、重心法の変形である。
# 2つのクラスターの重心の間の重み付けを求める時、重みを等しくして求めた距離を、
# 2つのクラスター間の距離とする

# (f) ウォード法
# ウォード法(Ward's method)は、最小分散法(minimum variance method)とも呼ばれる
# 2つのクラスターを融合する際に、グループ内の分散に対するグループ間の分散を最大化する基準で
# クラスターを形成していく方法である


# ●5-2-4 階層的クラスター分析のケーススタディ
# (1)hclust関数
# 階層的クラスター分析にはhclust関数を使う
# シンタックスは以下の通り

# hclust(d, method = 'complete', ...)

# 引数dは距離構造のデータを表す
# 引数methodは、先に見てきたクラスター分析の種類を指定する
# 以下が引数methodで設定できるクラスター分析である

# single  = 最近隣法
# complete  = 最遠隣法
# average  = 郡平均法
# centroid  = 重心法
# median  = メディアン法
# ward  = ウォード法
# mcquitty  = McQuitty法

# ・hclust関数の関連関数
# summary関数     結果のオブジェクトのリストを返す
# plot関数        樹形図を作成する
# plclust関数     樹形図を作成する
# cutree関数      クラスター(房)の数を指定し、グループ分けする
# cophenetic関数  コーフェン行列を返す


# (2)解析と結果
# まずユークリッド距離を用いた使用例を以下に示す
# データは先のseisekiを使う

seiseki.d <- dist(seiseki)
(sei.hc <- hclust(seiseki.d))

# 返された結果から用いたクラスター分析はcomplete(最遠隣法)、距離はeuclidena(ユークリッド)であることが分かる
# summary関数をつかって、結果のオブジェクトに格納されたデータのリストを呼び出せる

summary(sei.hc)

# $mergeにはクラスター分析の履歴(個体数 - 1)行2列の行列で記録される
# このデータはクラスターを形成する階層の情報である
# マイナス記号がついているのが個体の番号であり、マイナス記号が付いていないのがクラスターの番号である
# 行番号がクラスター形成の順番である

# 今回、使った最遠隣法では、ステップ1(第1行)は個体2(佐藤)と個体6(吉野)がクラスター1を形成する
# ステップ2(第2行)は個体3(鈴木)と個体5(川端)がクラスター2を形成する
# ステップ3(第3行)は個体1(田中)とクラスター2が新しいクラスター3を形成する

# $heightにはクラスター形成する樹形図の枝の長さ(高さ)が記録されている

sei.hc$height

# この値は$mergeの結果と対応している
# 例えば個体2(佐藤)、個体6(吉野)の距離は、12.40967である

# $orderには樹形図の左から右方向の個体の番号が記録される
# $labelsには個体のラベルが記録される
# $methodには、用いたクラスター分析の方法が記録される
# $callには、用いたhclust関数の書式が記録される
# $dist.methodには用いた距離の方法の名称が記録される

sei.hc$order

# ・樹形図の作成
# クラスター分析は、樹形図を描くことでより直観的にクラスターを把握することが出来る
# 樹形図を描くにはplot関数を使う

plot(sei.hc)

# 引数hang = -1で葉の高さを揃えることが出来る
plot(sei.hc, hang = -1)


# ・重心法とウォード法による樹形図
# 重心法とウォード法による樹形図を示すと以下のコードになる
# ウォード法ではキャンベラ距離(canberra)を使っている

plot(hclust(seiseki.d, 'centroid'), hang = -1)

plot(hclust(dist(seiseki, 'canberra'), 'ward.D'), hang = -1)

# 用いる距離、クラスター分析によって結果が大きく異なる場合も有る

# ・cutree関数
# cutree関数を使うことで、各個体がどのクラスターに属するかに関する情報が返される
# 引数k = 2はクラスター数の設定である

cutree(sei.hc, k = 2)

# この例では、2つのクラスターのどちらに所属するかを返している
# これは最遠距離法とユークリッド距離を用いた場合、全体を2つのクラスターに分けるとしたら、
# 田中、鈴木、本田、川端がクラスター1、佐藤、吉野、斉藤が1つのクラスターになることを意味している

# ●コーフェン行列
# 階層的クラスター分析の樹形図は、コーフェン行列に基づいている
# 距離行列から求めたコーフェン行列は、cophenetic関数で返すことが出来る

cophenetic(sei.hc)

# コーフェン行列は用いる方法によって結果が異なる
# そこで異なる方法による結果の中で、どの結果を用いるかが問題になる
# クラスター分析の結果の妥当性の評価に、距離の行列とコーフェン行列の相関係数(コーフェン相関係数)が用いられている
# コーフェン相関係数は、用いた方法で生成されたコーフェン行列と用いた距離の行列とのピアソン相関係数である
# 以下にユークリッド距離と最近遠法によるコーフェン相関係数を求める例を示す

cor(seiseki.d, cophenetic(sei.hc))

# コーフェン相関係数の値が大きいほど、距離行列と用いた方法のコーフェン行列との歪みが少ないと判断するが、
# 歪みが少ないことと分類の結果が妥当であることはイコールではない
# 経験上、ウォード法は妥当と思われる結果を返す確率が高いが、コーフェン相関係数が比較的低い値を返す

# ・コーフェン相関係数
# 方法              キャンベラ距離        ユークリッド距離
# 最近隣法(single)      0.875               0.892
# 最遠隣法(complete)    0.878               0.894
# 郡平均法(average)     0.889               0.900
# 重心法(centroid)      0.863               0.828
# メディアン法(median)  0.868               0.831
# ウォード法(ward)      0.855               0.863

# ウォード法は比較的低いコーフェン相関係数を返す
# これはウォード法では、その他の距離と異なり分散の情報を使うためである

# ここではirisのデータから2種類(versicolorとvirginica)のデータの一部を取り出して、キャンベラ距離の郡平均法とウォード法の結果を示す
# 2つのクラスターに分けることを前提にすると、ウォード法がより妥当な結果を出していると言える
# しかし、コーフェン相関係数はウォード法が郡平均法より低い

d <- dist(iris[c(51:60, 141:150), 1:4], 'can')

ha <- hclust(d, 'average')
hw <- hclust(d, 'ward.D')

cor(cophenetic(ha), d)
cor(cophenetic(hw), d)

plot(ha, main = 'キャンベラ距離, 郡平均法, cop - cor = 0.8057', cex = 1.2, hang = -1)
plot(hw, main = 'キャンベラ距離, ウォード法, cop - cor = 0.608', cex = 1.2, hang = -1)

# 階層的クラスター分析は、データ構造が複雑になると少数個の個体を入れ替えるだけで
# 結果が大きく変わることも珍しくない
# どの結果を採用するかに関しては、階層的クラスター分析の結果だけでなく、
# 他のデータ解析方法も用いて探索的に様々な角度からデータを眺め、総合的に判断することが必要になる


# 以上